<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="11:08AM EDT - Welcome to Hot Chips! This is the annual conference all about the latest, greatest, and upcoming big silicon that gets us all excited. Stay tuned during Monday and Tuesday for our regular AnandTech Live Blogs."><meta name=generator content="Hugo 0.98.0"><meta name=robots content="index,follow,noarchive"><title>Machine Learning (Esperanto, Enflame, Qualcomm) &#183;</title><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/pure-min.css><!--[if lte IE 8]><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-old-ie-min.css><![endif]--><!--[if gt IE 8]><!--><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-min.css><!--<![endif]--><!--[if lte IE 8]><link rel=stylesheet href=https://assets.cdnweb.info/hugo/blackburn/css/side-menu-old-ie.css><![endif]--><!--[if gt IE 8]><!--><link rel=stylesheet href=https://assets.cdnweb.info/hugo/blackburn/css/side-menu.css><!--<![endif]--><link rel=stylesheet href=https://assets.cdnweb.info/hugo/blackburn/css/blackburn.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=Raleway&display=swap" rel=stylesheet type=text/css><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.6.0/styles/androidstudio.min.css><script async src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.6.0/highlight.min.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel="shortcut icon" href=./img/favicon.ico type=image/x-icon></head><body><div id=layout><a href=#menu id=menuLink class=menu-link><span></span></a><div id=menu><a class="pure-menu-heading brand" href=./index.html>RaveDash</a><div class=pure-menu><ul class=pure-menu-list><li class=pure-menu-item><a class=pure-menu-link href=./index.html><i class="fa fa-home fa-fw"></i>Home</a></li><li class=pure-menu-item><a class=pure-menu-link href=./post/index.html><i class="fa fa-list fa-fw"></i>Posts</a></li><li class=pure-menu-item><a class=pure-menu-link href=./sitemap.xml><i class="fa fa-user fa-fw"></i>Sitemap</a></li><li class=pure-menu-item><a class=pure-menu-link href=./index.xml><i class="fa fa-phone fa-fw"></i>RSS</a></li></ul></div><div class="pure-menu social"><ul class=pure-menu-list></ul></div><div><div class=small-print><small>&copy; 2022. All rights reserved.</small></div><div class=small-print><small>Built with&nbsp;<a href=https://gohugo.io/ target=_blank>Hugo</a></small>
<small>Theme&nbsp;<a href=https://github.com/yoshiharuyamashita/blackburn target=_blank>Blackburn</a></small></div></div></div><div id=main><div class=header><h1>Machine Learning (Esperanto, Enflame, Qualcomm)</h1><h2>11:08AM EDT - Welcome to Hot Chips! This is the annual conference all about the latest, greatest, and upcoming big silicon that gets us all excited. Stay tuned during Monday and Tuesday for our regular AnandTech Live Blogs.</h2></div><div class=content><div class=post-meta><div><i class="fa fa-calendar fa-fw"></i>
<time>25 Apr 2024, 00:00</time></div></div><p><a id=post0824110823 href=#><span class=lb_time>11:08AM EDT</span></a> - Welcome to Hot Chips! This is the annual conference all about the latest, greatest, and upcoming big silicon that gets us all excited. Stay tuned during Monday and Tuesday for our regular AnandTech Live Blogs.</p><p><a id=post0824110857 href=#><span class=lb_time>11:08AM EDT</span></a> - Event starts at 8:30am PT, so in about 22 minutes</p><p><a id=post0824112533 href=#><span class=lb_time>11:25AM EDT</span></a> - Starting here in about 5 minutes</p><p><a id=post0824113012 href=#><span class=lb_time>11:30AM EDT</span></a> - First up is a talk from Esperanto Technologies</p><p><a id=post0824113104 href=#><span class=lb_time>11:31AM EDT</span></a> - AI Accelerator - 1000 RISC-V cores on a chip</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/490703203_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/490726859_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824113246 href=#><span class=lb_time>11:32AM EDT</span></a> - 1088 RISC-V cores</p><p><a id=post0824113253 href=#><span class=lb_time>11:32AM EDT</span></a> - ET-Minion with tensor units</p><p><a id=post0824113302 href=#><span class=lb_time>11:33AM EDT</span></a> - 160 million bytes of SRAM onboard</p><p><a id=post0824113307 href=#><span class=lb_time>11:33AM EDT</span></a> - PCIe x8 Gen 4</p><p><a id=post0824113314 href=#><span class=lb_time>11:33AM EDT</span></a> - Up to 200 Tera-Ops</p><p><a id=post0824113327 href=#><span class=lb_time>11:33AM EDT</span></a> - Under 20 watts for inference</p><p><a id=post0824113338 href=#><span class=lb_time>11:33AM EDT</span></a> - focus on recommendation models</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/490726859_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824113405 href=#><span class=lb_time>11:34AM EDT</span></a> - traditionally run on x86</p><p><a id=post0824113411 href=#><span class=lb_time>11:34AM EDT</span></a> - these servers need add-in cards</p><p><a id=post0824113419 href=#><span class=lb_time>11:34AM EDT</span></a> - Low power budget per card</p><p><a id=post0824113432 href=#><span class=lb_time>11:34AM EDT</span></a> - Multiple data type support</p><p><a id=post0824113438 href=#><span class=lb_time>11:34AM EDT</span></a> - dense and sparse workloads</p><p><a id=post0824113441 href=#><span class=lb_time>11:34AM EDT</span></a> - be programmable</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/490880312_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824113528 href=#><span class=lb_time>11:35AM EDT</span></a> - reduce off-die memory references</p><p><a id=post0824113603 href=#><span class=lb_time>11:36AM EDT</span></a> - Fixed function hardware can quickly become obsolete</p><p><a id=post0824113757 href=#><span class=lb_time>11:37AM EDT</span></a> - thousands of threads</p><p><a id=post0824113809 href=#><span class=lb_time>11:38AM EDT</span></a> - limited parallelism with single big chips</p><p><a id=post0824113816 href=#><span class=lb_time>11:38AM EDT</span></a> - 1000s of RISC-V cores in esperanto</p><p><a id=post0824113823 href=#><span class=lb_time>11:38AM EDT</span></a> - Large chips have large power</p><p><a id=post0824113831 href=#><span class=lb_time>11:38AM EDT</span></a> - Esperanto splits it across chips</p><p><a id=post0824113839 href=#><span class=lb_time>11:38AM EDT</span></a> - allows for lower voltage, increasing efciciency</p><p><a id=post0824113858 href=#><span class=lb_time>11:38AM EDT</span></a> - Highest recommendation performance inside 120W in six chips</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491102156_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491168703_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824114042 href=#><span class=lb_time>11:40AM EDT</span></a> - TSMC 7nm FinFET</p><p><a id=post0824114049 href=#><span class=lb_time>11:40AM EDT</span></a> - drive down voltage per core</p><p><a id=post0824114055 href=#><span class=lb_time>11:40AM EDT</span></a> - C dynamic is hard</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491229703_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491266187_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824114150 href=#><span class=lb_time>11:41AM EDT</span></a> - Efficiency vs voltage - 0.34 is best</p><p><a id=post0824114200 href=#><span class=lb_time>11:42AM EDT</span></a> - Inferences per second per watt</p><p><a id=post0824114219 href=#><span class=lb_time>11:42AM EDT</span></a> - One chip could use 275W at peak</p><p><a id=post0824114233 href=#><span class=lb_time>11:42AM EDT</span></a> - 0.75 volts is 164W per chip</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491337062_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824114311 href=#><span class=lb_time>11:43AM EDT</span></a> - Best efficient point is at 8.5 W - 2.5x better perf than at 0.9 volts</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491362734_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491407234_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824114425 href=#><span class=lb_time>11:44AM EDT</span></a> - 64-bit risc-v processor, software configurable l1 data cache</p><p><a id=post0824114433 href=#><span class=lb_time>11:44AM EDT</span></a> - in order pipeline</p><p><a id=post0824114437 href=#><span class=lb_time>11:44AM EDT</span></a> - SMT2</p><p><a id=post0824114501 href=#><span class=lb_time>11:45AM EDT</span></a> - 300 MHz to 2 GHz</p><p><a id=post0824114511 href=#><span class=lb_time>11:45AM EDT</span></a> - can do 64 ops on one tensor instruction</p><p><a id=post0824114514 href=#><span class=lb_time>11:45AM EDT</span></a> - 64k ops</p><p><a id=post0824114554 href=#><span class=lb_time>11:45AM EDT</span></a> - 512-bit wide integer per cycle, 256-bit wide FP per cycle, per core</p><p><a id=post0824114624 href=#><span class=lb_time>11:46AM EDT</span></a> - 8 cores on a chip form a neighborhood</p><p><a id=post0824114632 href=#><span class=lb_time>11:46AM EDT</span></a> - before wide length became a problem</p><p><a id=post0824114642 href=#><span class=lb_time>11:46AM EDT</span></a> - 8 minions share a single large instruction cache</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491557890_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824114655 href=#><span class=lb_time>11:46AM EDT</span></a> - far more efficient than having each core with its own I-cache</p><p><a id=post0824114708 href=#><span class=lb_time>11:47AM EDT</span></a> - cooperative loads</p><p><a id=post0824114729 href=#><span class=lb_time>11:47AM EDT</span></a> - custom instructions</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491625453_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824114747 href=#><span class=lb_time>11:47AM EDT</span></a> - 4 neighborhoods makes a shire</p><p><a id=post0824114754 href=#><span class=lb_time>11:47AM EDT</span></a> - with 4 MB of shared SRAM</p><p><a id=post0824114802 href=#><span class=lb_time>11:48AM EDT</span></a> - mesh interconnect on each shire</p><p><a id=post0824114818 href=#><span class=lb_time>11:48AM EDT</span></a> - SRAM banks could be partitioned as private L2 or shared L3</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491673203_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824114830 href=#><span class=lb_time>11:48AM EDT</span></a> - Meshes run over the cores</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491678046_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824114856 href=#><span class=lb_time>11:48AM EDT</span></a> - 16 LPDDR4X controllers</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491721343_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824114919 href=#><span class=lb_time>11:49AM EDT</span></a> - 256-bit wide LPDDR4X</p><p><a id=post0824114936 href=#><span class=lb_time>11:49AM EDT</span></a> - Six chips and 24 LPDDR4 chips on a PCIe card with a PCIe switch</p><p><a id=post0824114946 href=#><span class=lb_time>11:49AM EDT</span></a> - 192 GB of accelerator memory</p><p><a id=post0824114958 href=#><span class=lb_time>11:49AM EDT</span></a> - 822 GB/s total memory bandwidth per PCIe card</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491779250_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491787375_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824115023 href=#><span class=lb_time>11:50AM EDT</span></a> - OCP versions</p><p><a id=post0824115046 href=#><span class=lb_time>11:50AM EDT</span></a> - How to deploy at scale</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491811859_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824115057 href=#><span class=lb_time>11:50AM EDT</span></a> - 6 chips have a single heatspreader</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491852062_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824115138 href=#><span class=lb_time>11:51AM EDT</span></a> - Software through many interfaces</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491878187_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824115205 href=#><span class=lb_time>11:52AM EDT</span></a> - Esperanto projected performance</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/491950093_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/492007640_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/492014531_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824115415 href=#><span class=lb_time>11:54AM EDT</span></a> - Four high-performance ET-Maxions</p><p><a id=post0824115428 href=#><span class=lb_time>11:54AM EDT</span></a> - Full RV64GC ISA</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/492038390_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824115444 href=#><span class=lb_time>11:54AM EDT</span></a> - 24 billion transistors, 570mm2, 89 mask layers</p><p><a id=post0824115457 href=#><span class=lb_time>11:54AM EDT</span></a> - First silicon in bring up</p><p><a id=post0824115512 href=#><span class=lb_time>11:55AM EDT</span></a> - A0 silicon in test</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/492088078_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824115532 href=#><span class=lb_time>11:55AM EDT</span></a> - Highest performance commercial RISC-V chip to date</p><p><a id=post0824115550 href=#><span class=lb_time>11:55AM EDT</span></a> - Early Access for qualified customers later in 2021</p><p><a id=post0824115657 href=#><span class=lb_time>11:56AM EDT</span></a> - Q*A time</p><p><a id=post0824115809 href=#><span class=lb_time>11:58AM EDT</span></a> - Q: External memory and IO power add above 20W - A: IOs are included. 20W includes DRAM and other components</p><p><a id=post0824120013 href=#><span class=lb_time>12:00PM EDT</span></a> - Q: Why not BF16? A: Natively it does, but BF16 would be expanded FP32 for compute and put to BF16 back in storage. Because we do inference - customer wants inference, doesn't need BF16</p><p><a id=post0824120137 href=#><span class=lb_time>12:01PM EDT</span></a> - Q: Data cache size for general purpose A: With area of 1000 cores, shift L1/L2 to multi-level is important. Special circuits - keep very robust voltage, need to use large SRAM for low voltage. 4 KB L1 gave a good hit rate with the L2 for performance</p><p><a id=post0824120213 href=#><span class=lb_time>12:02PM EDT</span></a> - Next talk is Enflame</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/492526328_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824120248 href=#><span class=lb_time>12:02PM EDT</span></a> - First Gen</p><p><a id=post0824120259 href=#><span class=lb_time>12:02PM EDT</span></a> - Designed 2018, launched 2019</p><p><a id=post0824120302 href=#><span class=lb_time>12:03PM EDT</span></a> - DTU 1.0</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/492549421_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824120327 href=#><span class=lb_time>12:03PM EDT</span></a> - 80 TF of BF16, 12nm FinFet, 14.1 billion transistors, 200 GB/s interconnect</p><p><a id=post0824120412 href=#><span class=lb_time>12:04PM EDT</span></a> - 16 lanes PCIe 4.0</p><p><a id=post0824120451 href=#><span class=lb_time>12:04PM EDT</span></a> - 300W</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/492666718_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824120510 href=#><span class=lb_time>12:05PM EDT</span></a> - 2 HBM2 at 512 GB/s</p><p><a id=post0824120544 href=#><span class=lb_time>12:05PM EDT</span></a> - 32 AI compute cores</p><p><a id=post0824120546 href=#><span class=lb_time>12:05PM EDT</span></a> - ip networkj</p><p><a id=post0824120554 href=#><span class=lb_time>12:05PM EDT</span></a> - 4 clusters of 8 tensor units</p><p><a id=post0824120603 href=#><span class=lb_time>12:06PM EDT</span></a> - 40 data transfer engines</p><p><a id=post0824120627 href=#><span class=lb_time>12:06PM EDT</span></a> - on chip network*</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/492763000_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824120639 href=#><span class=lb_time>12:06PM EDT</span></a> - VLIW programmable</p><p><a id=post0824120643 href=#><span class=lb_time>12:06PM EDT</span></a> - 1024-bit bus with</p><p><a id=post0824120648 href=#><span class=lb_time>12:06PM EDT</span></a> - 256 KB of L1-Data</p><p><a id=post0824120655 href=#><span class=lb_time>12:06PM EDT</span></a> - DMA engine with 1 KB interface</p><p><a id=post0824120703 href=#><span class=lb_time>12:07PM EDT</span></a> - GPU-Care 1.0</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/492819656_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824120739 href=#><span class=lb_time>12:07PM EDT</span></a> - 256 Tensor compute Kernels</p><p><a id=post0824120757 href=#><span class=lb_time>12:07PM EDT</span></a> - Each kernel supports 1x-32bit MAC or 4x16-bit/8-bit MAC. All kernels do all precisions</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/492852140_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824120831 href=#><span class=lb_time>12:08PM EDT</span></a> - Introduce sparsity for power</p><p><a id=post0824120848 href=#><span class=lb_time>12:08PM EDT</span></a> - can fully skip instructions if zero power instruction detected</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/492908718_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824120922 href=#><span class=lb_time>12:09PM EDT</span></a> - 2 kbit per cycle for store, 1 kbit per cycle for load</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/492936156_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/492951015_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824120950 href=#><span class=lb_time>12:09PM EDT</span></a> - Cector and Scalar support sum and pooling</p><p><a id=post0824121047 href=#><span class=lb_time>12:10PM EDT</span></a> - hardware can add padding elements to get best efficiency compined with zero power instruction detection</p><p><a id=post0824121108 href=#><span class=lb_time>12:11PM EDT</span></a> - 256 kernels support convolution operations</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493033968_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493095546_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493102562_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824121220 href=#><span class=lb_time>12:12PM EDT</span></a> - Support various tensor shapes</p><p><a id=post0824121234 href=#><span class=lb_time>12:12PM EDT</span></a> - have to have it on a power of two boundary</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493132312_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493141156_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493165750_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824121330 href=#><span class=lb_time>12:13PM EDT</span></a> - L0 cache with 10 TB/s bandwidth</p><p><a id=post0824121348 href=#><span class=lb_time>12:13PM EDT</span></a> - Async data flow and compute pipeline</p><p><a id=post0824121401 href=#><span class=lb_time>12:14PM EDT</span></a> - 4D tensors</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493215296_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493230531_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824121426 href=#><span class=lb_time>12:14PM EDT</span></a> - Supports dimension reshape</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493248312_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824121504 href=#><span class=lb_time>12:15PM EDT</span></a> - 200 GB/s bi-directional IO per card</p><p><a id=post0824121517 href=#><span class=lb_time>12:15PM EDT</span></a> - custom protocol with sub-microsecond latency</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493321031_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824121559 href=#><span class=lb_time>12:15PM EDT</span></a> - Layer cables to racks without DMA</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493334343_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824121611 href=#><span class=lb_time>12:16PM EDT</span></a> - AIC and OAM</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493377453_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824121713 href=#><span class=lb_time>12:17PM EDT</span></a> - Scale up to 2D torus pod</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493451796_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824121810 href=#><span class=lb_time>12:18PM EDT</span></a> - Performance taken up to 160 card cluster</p><p><a id=post0824122007 href=#><span class=lb_time>12:20PM EDT</span></a> - Next product ready soon</p><p><a id=post0824122010 href=#><span class=lb_time>12:20PM EDT</span></a> - Q&A</p><p><a id=post0824122106 href=#><span class=lb_time>12:21PM EDT</span></a> - Q: Is there a training workload targeted? A: Training, supported vision, and machine language processing. First customer used MLP</p><p><a id=post0824122155 href=#><span class=lb_time>12:21PM EDT</span></a> - Q: Why design your own chip-to-chip protocol? Is it cache coherent A:It's not cache coherent, data sync mailbox. we wanted a lighter protocol with better latency</p><p><a id=post0824122225 href=#><span class=lb_time>12:22PM EDT</span></a> - Q: Sell to the west? A: Curretnly customers are Asia, but if you have interest, come to Enflame</p><p><a id=post0824122244 href=#><span class=lb_time>12:22PM EDT</span></a> - Next talk is Qualcomm Cloud AI 100</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493759156_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824122318 href=#><span class=lb_time>12:23PM EDT</span></a> - 12 TOPS/watt</p><p><a id=post0824122333 href=#><span class=lb_time>12:23PM EDT</span></a> - high performance and efficient accelerator</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493789125_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824122352 href=#><span class=lb_time>12:23PM EDT</span></a> - Another intro into what's driving AI</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493845562_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824122452 href=#><span class=lb_time>12:24PM EDT</span></a> - Qualcomm at the forefront of AI research, currently on 6th gen</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493878437_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824122531 href=#><span class=lb_time>12:25PM EDT</span></a> - two form factor - high performance in PCIe HHHL, and a more powereffcient dual M.2</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/493919609_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824122559 href=#><span class=lb_time>12:25PM EDT</span></a> - top level SoC slide</p><p><a id=post0824122607 href=#><span class=lb_time>12:26PM EDT</span></a> - bespoke high performance architecture</p><p><a id=post0824122617 href=#><span class=lb_time>12:26PM EDT</span></a> - 400+ Int8 TOPs</p><p><a id=post0824122621 href=#><span class=lb_time>12:26PM EDT</span></a> - 8 lanes of PCIe 4.0</p><p><a id=post0824122630 href=#><span class=lb_time>12:26PM EDT</span></a> - 16 GB/sof LPDDR4</p><p><a id=post0824122640 href=#><span class=lb_time>12:26PM EDT</span></a> - store all the weights on the SoC with 144 MB of on-chip memory</p><p><a id=post0824122716 href=#><span class=lb_time>12:27PM EDT</span></a> - Dual M.2 is for power</p><p><a id=post0824122728 href=#><span class=lb_time>12:27PM EDT</span></a> - power management controller</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/494023609_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824122738 href=#><span class=lb_time>12:27PM EDT</span></a> - 4-way VLIW</p><p><a id=post0824122744 href=#><span class=lb_time>12:27PM EDT</span></a> - 1800+ instructions</p><p><a id=post0824122748 href=#><span class=lb_time>12:27PM EDT</span></a> - SMT scalar core</p><p><a id=post0824122758 href=#><span class=lb_time>12:27PM EDT</span></a> - FP32/FP16 and INT16/INT8</p><p><a id=post0824122807 href=#><span class=lb_time>12:28PM EDT</span></a> - 1 MB of L2 cache</p><p><a id=post0824122825 href=#><span class=lb_time>12:28PM EDT</span></a> - Vector unit, Tensor unit</p><p><a id=post0824122844 href=#><span class=lb_time>12:28PM EDT</span></a> - Vector Tightly Coupled Memory 8 MB between all units</p><p><a id=post0824122852 href=#><span class=lb_time>12:28PM EDT</span></a> - almost all</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/494131125_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824122930 href=#><span class=lb_time>12:29PM EDT</span></a> - Can run at various power levels</p><p><a id=post0824122944 href=#><span class=lb_time>12:29PM EDT</span></a> - 12W for edge, 20W for ADAS, 70W High Perf mode</p><p><a id=post0824122950 href=#><span class=lb_time>12:29PM EDT</span></a> - 7nm</p><p><a id=post0824123001 href=#><span class=lb_time>12:30PM EDT</span></a> - Tensor unit is 5x more efficient than the Vecotr unit</p><p><a id=post0824123022 href=#><span class=lb_time>12:30PM EDT</span></a> - 16 AI cores</p><p><a id=post0824123057 href=#><span class=lb_time>12:30PM EDT</span></a> - 5 TOPs/W at high performance</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/494272906_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824123153 href=#><span class=lb_time>12:31PM EDT</span></a> - Full stack for inference</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/494359562_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824123317 href=#><span class=lb_time>12:33PM EDT</span></a> - Compiler supports mixed precision</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/494400781_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/494540656_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824123621 href=#><span class=lb_time>12:36PM EDT</span></a> - Optimizations for low power</p><p><a id=post0824123635 href=#><span class=lb_time>12:36PM EDT</span></a> - minimize DDR accesses and improve performance</p><p><a id=post0824123649 href=#><span class=lb_time>12:36PM EDT</span></a> - Reuse data as much as you can to begin before going to get more</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/494715343_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824123915 href=#><span class=lb_time>12:39PM EDT</span></a> - Split a netowrk across multiple AI100 cards</p><p><a id=post0824123927 href=#><span class=lb_time>12:39PM EDT</span></a> - up to 16 cards per system</p><p><a id=post0824123939 href=#><span class=lb_time>12:39PM EDT</span></a> - PCIe switch for peer-to-peer</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/494806437_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824124121 href=#><span class=lb_time>12:41PM EDT</span></a> - Performance at INT8 and Mixed, all inference</p><p><a id=post0824124218 href=#><span class=lb_time>12:42PM EDT</span></a> - 'industry leading performance numbers'</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/494912859_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824124243 href=#><span class=lb_time>12:42PM EDT</span></a> - Performance vs batch size</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/495001734_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824124404 href=#><span class=lb_time>12:44PM EDT</span></a> - AIMET can do inlfight compression for inference</p><p><a id=post0824124448 href=#><span class=lb_time>12:44PM EDT</span></a> - 15% increase in ResNET50 perf for only 1.1% reduction in accuracy</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/495068343_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824124510 href=#><span class=lb_time>12:45PM EDT</span></a> - Edge deployment vs server deployment</p><p><a id=post0824124524 href=#><span class=lb_time>12:45PM EDT</span></a> - DM.2e = dual M.2</p><p><a id=post0824124533 href=#><span class=lb_time>12:45PM EDT</span></a> - 15W TDP in that dual M.2</p><p><a href=#><img src=https://cdn.statically.io/img/images.anandtech.com/doci/16906/495115984_575px.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p><a id=post0824124609 href=#><span class=lb_time>12:46PM EDT</span></a> - Scalable solution for 5G, ADAS, infrastructure</p><p><a id=post0824124614 href=#><span class=lb_time>12:46PM EDT</span></a> - Q&A time</p><p><a id=post0824124718 href=#><span class=lb_time>12:47PM EDT</span></a> - Q: Are the power points static or automatic adjustment A: Chip has DVFS - based on power can change DVFS. For TDP, based on solution you can set TDP in firmware</p><p><a id=post0824124745 href=#><span class=lb_time>12:47PM EDT</span></a> - Q: 12 TOPS/W based board-level or chip-level? A: Chip</p><p><a id=post0824124920 href=#><span class=lb_time>12:49PM EDT</span></a> - Q: What are main drivers to achieve Tops/W A: Good building blocks - 6th gen. Been in business a long time. Been doing it in cell phones a long time, especially inference. Basic block is efficient. VLIW - compiler is doing a fair bit of lifting, keeping hardware simpler. Same process for SoC level. Not cache coherent, enabled through compiler</p><p><a id=post0824125121 href=#><span class=lb_time>12:51PM EDT</span></a> - Q: Tradeoffs between VLIW vs RISC A: ML fits very well on VLIW, have insights. We know how to do very efficient VLIW cores. But workload is well suited for VLIW. Did evaluation, but found this was the best way.</p><p><a id=post0824125148 href=#><span class=lb_time>12:51PM EDT</span></a> - Q: NOC details? Mesh, crossbar? A: Hybrid, more linear with routers</p><p><a id=post0824125318 href=#><span class=lb_time>12:53PM EDT</span></a> - Q: Systolic array? A: No</p><p><a id=post0824125332 href=#><span class=lb_time>12:53PM EDT</span></a> - Q: Scalar core is RISCV A: Proprietary VLIW</p><p><a id=post0824125504 href=#><span class=lb_time>12:55PM EDT</span></a> - That's a wrap</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH53hY9vZqGnpGKwqbXPrGRraGJmeq211Z5km6SfnHqurcKhoKedXaGyor7NoqWgZZWovaa%2BwKerqGWVo7OtrcyeZKqtkaGwsLnM</p><h4><i class="fas fa-share-alt" aria-hidden=true></i>&nbsp;Share!</h4><ul class=share-buttons><li><a href="https://www.facebook.com/sharer/sharer.php?u=%2fhot-chips-2021-live-blog-machine-learning-esperanto-enflame-qualcomm.html" target=_blank title="Share on Facebook"><i class="fab fa-facebook" aria-hidden=true></i><span class=sr-only>Share on Facebook</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="https://twitter.com/intent/tweet?source=%2fhot-chips-2021-live-blog-machine-learning-esperanto-enflame-qualcomm.html" target=_blank title=Tweet><i class="fab fa-twitter" aria-hidden=true></i><span class=sr-only>Tweet</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="https://plus.google.com/share?url=%2fhot-chips-2021-live-blog-machine-learning-esperanto-enflame-qualcomm.html" target=_blank title="Share on Google+"><i class="fab fa-google-plus" aria-hidden=true></i><span class=sr-only>Share on Google+</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="http://www.tumblr.com/share?v=3&u=%2fhot-chips-2021-live-blog-machine-learning-esperanto-enflame-qualcomm.html" target=_blank title="Post to Tumblr"><i class="fab fa-tumblr" aria-hidden=true></i><span class=sr-only>Post to Tumblr</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="http://pinterest.com/pin/create/button/?url=%2fhot-chips-2021-live-blog-machine-learning-esperanto-enflame-qualcomm.html" target=_blank title="Pin it"><i class="fab fa-pinterest-p" aria-hidden=true></i><span class=sr-only>Pin it</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="http://www.reddit.com/submit?url=%2fhot-chips-2021-live-blog-machine-learning-esperanto-enflame-qualcomm.html" target=_blank title="Submit to Reddit"><i class="fab fa-reddit-alien" aria-hidden=true></i><span class=sr-only>Submit to Reddit</span></a></li></ul><style>ul.share-buttons{list-style:none;padding:0}ul.share-buttons li{display:inline}ul.share-buttons .sr-only{position:absolute;clip:rect(1px 1px 1px 1px);clip:rect(1px,1px,1px,1px);padding:0;border:0;height:1px;width:1px;overflow:hidden}</style><div class="prev-next-post pure-g"><div class=pure-u-1-24 style=text-align:left><a href=./leaked-cia-document-dark-knight-rises-html.html><i class="fa fa-chevron-left"></i></a></div><div class=pure-u-10-24><nav class=prev><a href=./leaked-cia-document-dark-knight-rises-html.html>Read a Leaked CIA Document (That Is Actually Part of a Viral Campaign for The Dark Knight Rise</a></nav></div><div class=pure-u-2-24>&nbsp;</div><div class=pure-u-10-24><nav class=next><a href=./hailey-baldwin-abs-crop-tops-pics-photos-3634164.html>Hailey Baldwins Abs In Crop Tops Pics Of The Model Hollywood Life</a></nav></div><div class=pure-u-1-24 style=text-align:right><a href=./hailey-baldwin-abs-crop-tops-pics-photos-3634164.html><i class="fa fa-chevron-right"></i></a></div></div></div></div></div><script src=https://assets.cdnweb.info/hugo/blackburn/js/ui.js></script>
<script src=https://assets.cdnweb.info/hugo/blackburn/js/menus.js></script>
<script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>